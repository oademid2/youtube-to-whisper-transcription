{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv # add this line\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch  # install steps: pytorch.org\n",
    "\n",
    "from tqdm.auto import tqdm  # !pip install tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = whisper.load_model(\"base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "import urllib.parse as p\n",
    "import pandas as pd\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import json\n",
    "def obj_dict(obj):\n",
    "    return obj.__dict__\n",
    "\n",
    "import subprocess\n",
    "import requests\n",
    "import re\n",
    "import youtube_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube  # !pip install pytube\n",
    "from pytube.exceptions import RegexMatchError\n",
    "from tqdm.auto import tqdm  # !pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable OAuthlib's HTTPS verification when running locally.\n",
    "# *DO NOT* leave this option enabled in production.\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = os.getenv('DEVELOPER_KEY')\n",
    "\n",
    "os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey = DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "\n",
    "# Use a service account.\n",
    "cred = credentials.Certificate('../analytics-c4f16-firebase-adminsdk-plgdk-2a3d65b3a8.json')\n",
    "\n",
    "app = firebase_admin.initialize_app(cred)\n",
    "\n",
    "db = firestore.client(app)\n",
    "batch = db.batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nested_dict_keys(dic, keys):\n",
    "    d = dic\n",
    "    for key in keys:\n",
    "        if key in d.keys():\n",
    "            d = d[key]\n",
    "            continue\n",
    "        else:\n",
    "            return dic['snippet']['thumbnails']['default']['url'] \n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseHelper():\n",
    "    \n",
    "    def __init__(self, db):\n",
    "        self.db = db\n",
    "        self.CHANNELS = None\n",
    "        self.VIDEOS = {}\n",
    "    \n",
    "    def get_channel_db(self,db):\n",
    "        docs = self.db.collection(u'channels').stream()\n",
    "        channelsCollection = {}\n",
    "        for doc in docs:\n",
    "            channelsCollection[doc.id]=doc.to_dict()\n",
    "        return channelsCollection\n",
    "    \n",
    "    def set_channels(self):\n",
    "        self.CHANNELS = self.get_channel_db(db)\n",
    "        \n",
    "    def set_videos(self,_id,videos):\n",
    "        self.VIDEOS[_id] = videos\n",
    "\n",
    "    def get_channel_videos(self,channelId):\n",
    "\n",
    "        docs = db.collection(u'Videos').order_by('publishedAt', direction=firestore.Query.DESCENDING).where(u\"channelID\",u\"==\",u\"{}\".format(channelId)).stream()\n",
    "\n",
    "        collection = []\n",
    "        for doc in docs:\n",
    "            vid =doc.to_dict()\n",
    "            if \"#shorts\" not in vid['title']:\n",
    "                collection.append(vid)\n",
    "        return collection\n",
    "    \n",
    "    def get_channel_doc(self, video_url, allChannels):\n",
    "        channel_id = video_url_to_channel_id(video_url)\n",
    "        print(channel_id)\n",
    "\n",
    "        for record in allChannels.values():\n",
    "            if record['channelId'] == channel_id:\n",
    "                print(\"Channel {} already exists.\".format(record['title']))\n",
    "                return record\n",
    "\n",
    "        channelToAdd = generate_channel_json(video_url)    \n",
    "\n",
    "\n",
    "        add = input(\"Create channel for url ? (0 for no, any other key for yes)\".format(channelToAdd['title']))\n",
    "\n",
    "        if add == 0:\n",
    "            print(\"No channel for url\")\n",
    "            return\n",
    "\n",
    "\n",
    "        docRef = db.collection(u\"channels\").document(u\"{}\".format(channelToAdd['channelId'])).set(channelToAdd)\n",
    "        print(\"Channel {} is created.\".format(channelToAdd['title']))\n",
    "        return channelToAd\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_channel_json(sourceValue, source=\"url\"):\n",
    "    \n",
    "    if source==\"id\":\n",
    "        channel_id = sourceValue\n",
    "    elif source==\"url\":\n",
    "        video_url = sourceValue\n",
    "        channel_id = video_url_to_channel_id(video_url)\n",
    "    else:\n",
    "        raise Exception(\"{} not a valid source.\".format(source))\n",
    "            \n",
    "    request = youtube.channels().list(\n",
    "            part=\"snippet,contentDetails\",\n",
    "            id=channel_id\n",
    "        )\n",
    "    response = request.execute()\n",
    "    \n",
    "    channel_df = {}#pd.DataFrame(columns=['channelId', 'title', 'uploads','description','thumbnail'])\n",
    "    channel_df['channelId'] = channel_id\n",
    "    channel_df['title'] = response['items'][0]['snippet']['title']\n",
    "    channel_df['uploads'] = response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    channel_df['description'] = response['items'][0]['snippet']['description']\n",
    "    channel_df['thumbnail'] = check_nested_dict_keys(response['items'][0],[\"snippet\",\"thumbnails\",\"high\",\"url\"])\n",
    "    channel_df['videos'] = []\n",
    "    \n",
    "    #full_channel_df = pd.concat([full_channel_df, pd.DataFrame.from_dict(channel_df)])\n",
    "\n",
    "    return channel_df\n",
    "\n",
    "def video_url_to_channel_id(video_url):\n",
    "    video_param_from_url = video_url.split(\"/watch?v=\")[1].split(\"&\")[0]\n",
    "    request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails\",\n",
    "            id=video_param_from_url\n",
    "        )\n",
    "    response = request.execute()\n",
    "    channel_id = response['items'][0]['snippet']['channelId']\n",
    "    return channel_id\n",
    "\n",
    "\n",
    "def get_channel_doc(video_url, allChannels):\n",
    "    channel_id = video_url_to_channel_id(video_url)\n",
    "    print(channel_id)\n",
    "    \n",
    "    for record in allChannels.values():\n",
    "        if record['channelId'] == channel_id:\n",
    "            print(\"Channel {} already exists.\".format(record['title']))\n",
    "            return record\n",
    "\n",
    "    channelToAdd = generate_channel_json(video_url)    \n",
    "\n",
    "\n",
    "    add = input(\"Create channel for url ? (0 for no, any other key for yes)\".format(channelToAdd['title']))\n",
    "\n",
    "    if add == 0:\n",
    "        print(\"No channel for url\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    docRef = db.collection(u\"channels\").document(u\"{}\".format(channelToAdd['channelId'])).set(channelToAdd)\n",
    "    print(\"Channel {} is created.\".format(channelToAdd['title']))\n",
    "    return channelToAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uploads_id(youtube, channel_id):\n",
    "    request = youtube.channels().list(\n",
    "            part=\"snippet,contentDetails\",\n",
    "            id=channel_id\n",
    "        )\n",
    "    response = request.execute()\n",
    "    return response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "\n",
    "def get_uploaded_videos_response(youtube, channel_id,**args):\n",
    "    \n",
    "    playlist_id = get_uploads_id(youtube, channel_id)\n",
    "    max_total=args.get(\"max_total\") if args.get(\"max_total\") else 1000\n",
    "    oldest_date=args.get(\"oldest_date\") if args.get(\"oldest_date\") else False\n",
    "\n",
    "    \n",
    "    params = {\n",
    "        \"part\":\"snippet,contentDetails\",\n",
    "        \"playlistId\": playlist_id,\n",
    "        \"maxResults\": 50\n",
    "    }\n",
    "    \n",
    "    uploaded_videos_content_details_list = []\n",
    "\n",
    "    while True:\n",
    "        response = youtube.playlistItems().list(**params).execute()\n",
    "        \n",
    "        \n",
    "        if oldest_date:\n",
    "                \n",
    "            oldestResponseDate = response.get('items')[-1]['snippet']['publishedAt'].split(\"T\")[0]\n",
    "            if oldestResponseDate <= oldest_date:\n",
    "                add_response = []\n",
    "                for r in response.get('items'):\n",
    "                    date = r['snippet']['publishedAt'].split(\"T\")[0]\n",
    "                    if date <= oldest_date:\n",
    "                        break\n",
    "                    else:\n",
    "                        add_response.append(r)\n",
    "                videos_list = uploaded_videos_content_details_list + add_response\n",
    "                print(\"New videos found: {}\".format(len(videos_list)))\n",
    "                print(\"Newest video: {}, {}\".format(videos_list[0]['snippet']['title'],videos_list[0]['snippet']['publishedAt']))\n",
    "                print(\"Oldest video: {}, {}\".format(videos_list[-1]['snippet']['title'],videos_list[-1]['snippet']['publishedAt']))\n",
    "\n",
    "\n",
    "                return videos_list\n",
    "\n",
    "      \n",
    "        uploaded_videos_content_details_list = uploaded_videos_content_details_list + response.get('items')\n",
    "        \n",
    "        if (max_total and len(uploaded_videos_content_details_list) >= max_total):\n",
    "            return uploaded_videos_content_details_list[:max_total]\n",
    "     \n",
    "        elif 'nextPageToken' in response.keys():\n",
    "            params['pageToken'] = response['nextPageToken']\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(\"Videos retreived.\")\n",
    "    return uploaded_videos_content_details_list\n",
    "\n",
    "def getYoutubeDuration(videoId):\n",
    "    responseVideoDetails = youtube.videos().list( part=\"contentDetails\",id=videoId).execute()\n",
    "    durationResponse=responseVideoDetails['items'][0]['contentDetails']['duration']\n",
    "\n",
    "    duration_string = durationResponse.replace('PT',\"\")\n",
    "    number_values = re.findall('\\d+',duration_string)\n",
    "    symbols_available= ''.join([i for i in duration_string if not i.isdigit()])\n",
    "    symbol_map = {}\n",
    "    for symbol in 'HMS':\n",
    "        index = symbols_available.find(symbol)\n",
    "        if index > -1:\n",
    "            symbol_map[symbol] = number_values[index]\n",
    "\n",
    "    duration = 0\n",
    "\n",
    "\n",
    "    for idx in symbol_map:\n",
    "        if idx == \"H\":\n",
    "            duration = int(symbol_map[idx])*60*60 + duration\n",
    "        if idx == \"M\":\n",
    "            duration = int(symbol_map[idx])*60 + duration\n",
    "        if idx == \"S\":\n",
    "            duration = int(symbol_map[idx]) + duration\n",
    "            \n",
    "    return duration\n",
    "\n",
    "def video_response_list_to_list_of_dicts(response_list):\n",
    "    \n",
    "    list_of_dicts = []\n",
    "    for x in response_list:\n",
    "        if \"#shorts\" in x['snippet']['title']:\n",
    "            continue\n",
    "            \n",
    "        video_df = {}\n",
    "        video_df['id'] = x['id']\n",
    "        video_df['videoId'] = x['contentDetails']['videoId'] \n",
    "        video_df['title'] = x['snippet']['title'] \n",
    "        video_df['description'] = x['snippet']['description'] \n",
    "        video_df['thumbnail'] = check_nested_dict_keys(x,[\"snippet\",\"thumbnails\",\"maxres\",\"url\"])      \n",
    "        video_df['channelID'] = x['snippet']['channelId'] \n",
    "        video_df['videoId'] = x['contentDetails']['videoId'] \n",
    "        video_df['publishedAt'] =x['snippet']['publishedAt'].split(\"T\")[0] \n",
    "        video_df['videoUrl'] = \"https://www.youtube.com/watch?v={}\".format(x['contentDetails']['videoId'] )\n",
    "        video_df['duration'] = getYoutubeDuration(x['contentDetails']['videoId']  )\n",
    "        \n",
    "        video_df['text'] = ''\n",
    "#         video_df['assemblySentences'] ='' \n",
    "#         video_df['assemblyText'] ='' \n",
    "#         video_df['availability']=''\n",
    "#         video_df['assemblyId'] = ''\n",
    "#         video_df['ytaTranscript'] = ''\n",
    "#         video_df['mainTranscript'] = ''\n",
    "#         video_df['ytaAvailability'] = ''\n",
    "\n",
    "\n",
    "        list_of_dicts.append(video_df)\n",
    "\n",
    "\n",
    "    return list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullChannel(url, DB, oldest_date=None):\n",
    "\n",
    "    #Create channel\n",
    "    channel = get_channel_doc(url,DB.CHANNELS)\n",
    "    \n",
    "    videos_collections= DB.get_channel_videos(channel['channelId'])#load all videos\n",
    "    \n",
    "    DB.set_videos(channel['channelId'],videos_collections)\n",
    "    #check for videos not yet uploaded\n",
    "\n",
    "    if videos_collections and oldest_date:\n",
    "        oldest_date = videos_collections[0]['publishedAt']\n",
    "    else:\n",
    "        oldest_date = None\n",
    "    new_video_responses = get_uploaded_videos_response(youtube, channel['channelId'],\n",
    "                                               oldest_date=oldest_date)\n",
    "\n",
    "    list_of_new_video_responses_as_dicts = video_response_list_to_list_of_dicts(new_video_responses)\n",
    "\n",
    "\n",
    "    return channel, list_of_new_video_responses_as_dicts\n",
    "\n",
    "#add videos to transcript execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCost(videoList):\n",
    "    cost = 0\n",
    "    for vid in videoList:\n",
    "        cost = vid['duration']*0.00025 + cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcript_object(video, text, result,source):\n",
    "    item = {}\n",
    "    item['videoId'] = video['videoId']\n",
    "    item['transcriptId'] = 'TRANSCRIPT_'+video['videoId']\n",
    "    \n",
    "    keys = ['start','end','text','id']\n",
    "    segments = result['segments']\n",
    "    item['transcript'] = [{ keep: item[keep] for keep in keys } for item,i in zip(segments,range(len(segments)) )]\n",
    "    item['fullTranscript'] = result\n",
    "    item['text'] = text\n",
    "    item['source'] = source\n",
    "\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(url,_id):\n",
    "    yt = YouTube(url)\n",
    "    video = yt.streams.filter(only_audio=True).first()\n",
    "    out_file=video.download(output_path=\"audio_files\")\n",
    "    base, ext = os.path.splitext(out_file)\n",
    "    new_file = 'audio_files/'+_id+'.mp3'\n",
    "    os.rename(out_file, new_file)\n",
    "    a = new_file\n",
    "    return a\n",
    "\n",
    "def transcribe(video):\n",
    "    \n",
    "    metric = {}\n",
    "    result = {}\n",
    "    _id = video['videoId']\n",
    "\n",
    "\n",
    "    try:\n",
    "        start = time.time()\n",
    "\n",
    "        get_audio(video['videoUrl'],_id)\n",
    "        print('Downloaded {}'.format(video['videoId']))\n",
    "\n",
    "\n",
    "        # transcribe to get speech-to-text data\n",
    "        result = model.transcribe('audio_files/{}.mp3'.format(_id))\n",
    "\n",
    "        # add results to data list\n",
    "        with open('transcript_files/{}.json'.format(_id), 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "        os.remove(\"audio_files/{}.mp3\".format(_id))\n",
    "        source = \"whisper\"\n",
    "        print('Transcribed {}'.format(video['videoId']))\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e: # work on python 3.x\n",
    "\n",
    "        print(\"video error: {}\".format(_id))\n",
    "\n",
    "     \n",
    "        try:\n",
    "            print('Could not get streaming data. Attempting assembly...')\n",
    "            ydl_opts = {'format': 'bestaudio'}\n",
    "\n",
    "\n",
    "            ydl =youtube_dl.YoutubeDL(ydl_opts)\n",
    "\n",
    "            videoUrl = video['videoUrl']\n",
    "            info = ydl.extract_info(videoUrl, download=False)\n",
    "            audioUrl = info['formats'][0]['url']\n",
    "            requestForTrancript = postTranscript(audioUrl)\n",
    "            source = \"pendng\"\n",
    "        except Exception as e: # work on python 3.x\n",
    "            print('Could not get assembly data.')\n",
    "\n",
    "            source = \"fail\"\n",
    "\n",
    "\n",
    "    \n",
    "    metric['id'] = _id\n",
    "    metric['time'] = time.time() - start\n",
    "    \n",
    "    return result, metric,source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_audio(\"https://www.youtube.com/watch?v=7hrSj5qkHv4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = DatabaseHelper(db)\n",
    "DB.set_channels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCamLstJyCa-t5gfZegxsFMw\n",
      "Channel Colin and Samir already exists.\n",
      "Videos retreived.\n"
     ]
    }
   ],
   "source": [
    "url= \"https://www.youtube.com/watch?v=lBCOOTyU46M&t=512s\"\n",
    "channel, new_videos = pullChannel(url,DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = DB.db.collection(u'Transcripts').stream()\n",
    "transcripts_list = []\n",
    "for doc in docs:\n",
    "    try:\n",
    "        transcripts_list.append(doc.to_dict()['videoId'])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-db7703a81864>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'7zd6EA5GdEM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone_1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdone_2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdone_3\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtranscripts_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_videos\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'videoId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'duration'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'duration'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m45\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgetCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0.00025\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-db7703a81864>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'7zd6EA5GdEM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone_1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdone_2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdone_3\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtranscripts_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_videos\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'videoId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'duration'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'duration'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m45\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgetCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0.00025\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not callable"
     ]
    }
   ],
   "source": [
    "# result, metric, source = transcribe(save)\n",
    "done_1 = ['gU24yrixXD8','irIN7SHvLDc','lBCOOTyU46M','iq71Cb2jEIE','iq71Cb2jEIE']\n",
    "done_2 = ['1t5oYKEn-1E', 'IjoTYJNr8DA', 'xp2VGAjHZWY', '3QZkjsMfj3I', 'gX0mrw1uZcY', 'NphQGsm4rvk', 'OE4ti4alRN8', 'w4iUuroktxw', 'yNLqaQ6slkw', 'N5YW4JB07-8', 'gGBCbswZbnI', '0bC1ah_x8zo', 'hYaGD0V2OkE', 'wkDlfvTed1c', '7qoe2qhcZ-Y', '88067BiKU4Y', 'jzMsnNxzejI', 'pvtMJFPyiLM', '3vbZvRHpM8w', '7hrSj5qkHv4', 'VbNIh88Nq5k', 'XuVR_elE1Pw', 'r8bzWKBvZsE', '9CxaZWkwHzE', 'x5lBJE2Ok8E', 'z_czmz_bJqk', 't69cK3Ih_Og', 'BB2HTaXTy0k', 'HmrjOq8epsg', 'MLyEDp7e0Q8', '9cn_r1z6zjo', 'o8UBXsiiS24', 'SpdWaOngRWM', 'jz_qFyTrS8w', 'Dh909TbYn7Y', 'G5c6qof96DM', 'UZSwDZ72Lp8', 'dbOXYhjpXAc', 'YVuIm8OLz-8', 'r-Y1LRtsFaU', '_TAxmgPQtzc']\n",
    "done_3 = ['nxyThD3-GTw','i05bI03nzv4','vwtRRdmZSYw', 'xIC1nS4DU6M', 'JoI4BRPd8us', 'PiGCHXt5eBs', '6OICqRSTRjY', '71Xz6bYoRGc', '7PIlp-FSN5I', 'vDGnnLLXGTo']\n",
    "error = ['7zd6EA5GdEM']\n",
    "done = done_1+done_2+done_3+error+transcripts_list\n",
    "l = [v for v in new_videos if ((v['videoId']not in done )and not (v['text'] == 'error') and (v['duration'] <= 45*60))]\n",
    "(getCost(l[:10])/0.00025/60/60)*0.5\n",
    "l1 = l[:5]\n",
    "l2 = l[5:10]\n",
    "l3 = l[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "t[0:5],t[5:10],t[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3834722222222229"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(getCost(l)/0.00025/60/60)*0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save = new_videos[6]\n",
    "# t = [v for v in new_videos if ((v['videoId']not in done )and (v['duration'] >0))]\n",
    "\n",
    "# (getCost(t)/0.00025/60/60)*0.5\n",
    "# result, metric, source = transcribe(save)\n",
    "# done_1 = ['gU24yrixXD8','irIN7SHvLDc','lBCOOTyU46M','iq71Cb2jEIE','iq71Cb2jEIE']\n",
    "# done_2 = ['1t5oYKEn-1E', 'IjoTYJNr8DA', 'xp2VGAjHZWY', '3QZkjsMfj3I', 'gX0mrw1uZcY', 'NphQGsm4rvk', 'OE4ti4alRN8', 'w4iUuroktxw', 'yNLqaQ6slkw', 'N5YW4JB07-8', 'gGBCbswZbnI', '0bC1ah_x8zo', 'hYaGD0V2OkE', 'wkDlfvTed1c', '7qoe2qhcZ-Y', '88067BiKU4Y', 'jzMsnNxzejI', 'pvtMJFPyiLM', '3vbZvRHpM8w', '7hrSj5qkHv4', 'VbNIh88Nq5k', 'XuVR_elE1Pw', 'r8bzWKBvZsE', '9CxaZWkwHzE', 'x5lBJE2Ok8E', 'z_czmz_bJqk', 't69cK3Ih_Og', 'BB2HTaXTy0k', 'HmrjOq8epsg', 'MLyEDp7e0Q8', '9cn_r1z6zjo', 'o8UBXsiiS24', 'SpdWaOngRWM', 'jz_qFyTrS8w', 'Dh909TbYn7Y', 'G5c6qof96DM', 'UZSwDZ72Lp8', 'dbOXYhjpXAc', 'YVuIm8OLz-8', 'r-Y1LRtsFaU', '_TAxmgPQtzc']\n",
    "# error = ['7zd6EA5GdEM']\n",
    "# done = done_1+done_2\n",
    "# nums = range(len(new_videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done_1 = ['gU24yrixXD8','irIN7SHvLDc','lBCOOTyU46M','iq71Cb2jEIE','iq71Cb2jEIE']\n",
    "# done_2 = ['1t5oYKEn-1E', 'IjoTYJNr8DA', 'xp2VGAjHZWY', '3QZkjsMfj3I', 'gX0mrw1uZcY', 'NphQGsm4rvk', 'OE4ti4alRN8', 'w4iUuroktxw', 'yNLqaQ6slkw', 'N5YW4JB07-8', 'gGBCbswZbnI', '0bC1ah_x8zo', 'hYaGD0V2OkE', 'wkDlfvTed1c', '7qoe2qhcZ-Y', '88067BiKU4Y', 'jzMsnNxzejI', 'pvtMJFPyiLM', '3vbZvRHpM8w', '7hrSj5qkHv4', 'VbNIh88Nq5k', 'XuVR_elE1Pw', 'r8bzWKBvZsE', '9CxaZWkwHzE', 'x5lBJE2Ok8E', 'z_czmz_bJqk', 't69cK3Ih_Og', 'BB2HTaXTy0k', 'HmrjOq8epsg', 'MLyEDp7e0Q8', '9cn_r1z6zjo', 'o8UBXsiiS24', 'SpdWaOngRWM', 'jz_qFyTrS8w', 'Dh909TbYn7Y', 'G5c6qof96DM', 'UZSwDZ72Lp8', 'dbOXYhjpXAc', 'YVuIm8OLz-8', 'r-Y1LRtsFaU', '_TAxmgPQtzc']\n",
    "# done_3 = ['vwtRRdmZSYw', 'xIC1nS4DU6M', 'JoI4BRPd8us', 'PiGCHXt5eBs', '6OICqRSTRjY', '71Xz6bYoRGc', '7PIlp-FSN5I', 'vDGnnLLXGTo']\n",
    "# error = ['7zd6EA5GdEM']\n",
    "# done = done_1+done_2+done_3+error\n",
    "#     l = [v for v in l if v['videoId'] == \"PDZ6sYal5A0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TranscribeList(l):\n",
    "    data = []\n",
    "    metrics= []\n",
    "    done = []\n",
    "    i=0\n",
    "    for doc in l:\n",
    "\n",
    "        if doc['videoId'] not in done:\n",
    "            print(i)\n",
    "        else:\n",
    "            print(\"already transcribed.\")\n",
    "            continue\n",
    "        i = i+1\n",
    "\n",
    "        result, metric, source = transcribe(doc)\n",
    "\n",
    "        if source == \"whisper\":\n",
    "            data.append(result)\n",
    "            metrics.append(metric)\n",
    "\n",
    "\n",
    "      \n",
    "            doc['text'] = result['text']\n",
    "            \n",
    "            transcript = transcript_object(doc, result['text'],result, source)\n",
    "\n",
    "            docRef = DB.db.collection(u\"Videos\").document(u\"{}\".format(doc['videoId'])).set(doc)\n",
    "            docRef = DB.db.collection(u\"Transcripts\").document(u\"{}\".format(doc['videoId'])).set(transcript)\n",
    "\n",
    "            done.append(doc)\n",
    "            print(metric['time'],\" -- \",doc[\"duration\"], \" -- \", print(metric['time']/doc[\"duration\"]))\n",
    "\n",
    "\n",
    "        else:\n",
    "            doc['text'] = 'error'\n",
    "            #transcript = transcript_object(doc, result['segments'], source)\n",
    "\n",
    "            docRef = DB.db.collection(u\"Videos\").document(u\"{}\".format(doc['videoId'])).set(doc)\n",
    "    #         docRef = DB.db.collection(u\"Transcripts\").document(u\"{}\".format(doc['videoId'])).set(transcript)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'error'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "video error: cbBxEmGOfk4\n",
      "Could not get streaming data. Attempting assembly...\n",
      "[youtube] cbBxEmGOfk4: Downloading webpage\n",
      "[youtube] cbBxEmGOfk4: Refetching age-gated info webpage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: unable to download video info webpage: HTTP Error 410: Gone\n",
      "ERROR: Sign in to confirm your age\n",
      "This video may be inappropriate for some users.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not get assembly data.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "TranscribeList([l[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "\n",
    "# ray.shutdown()\n",
    "# ray.init()\n",
    "\n",
    "# # Start two tasks in the background.\n",
    "# x_id = TranscribeList.remote(l1)\n",
    "# y_id = TranscribeList.remote(l2)\n",
    "# z_id = TranscribeList.remote(l3)\n",
    "\n",
    "\n",
    "# # Block until the tasks are done and get the results.\n",
    "# x, y, z = ray.get([x_id, y_id, z_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "# # import generate_transcripts_script as defs\n",
    "# pool = Pool()\n",
    "\n",
    "# result1 = pool.apply_async(TranscribeList, [l1,\"A\"])    # evaluate \"solve1(A)\" asynchronously\n",
    "# result2 = pool.apply_async(TranscribeList, [l2,\"B\"])    # evaluate \"solve2(B)\" asynchronously\n",
    "# result3 = pool.apply_async(TranscribeList, [l3,\"C\"])    # evaluate \"solve2(B)\" asynchronously\n",
    "\n",
    "# answer1 = result1.get()\n",
    "# answer2 = result2.get()\n",
    "# answer3 = result3.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
