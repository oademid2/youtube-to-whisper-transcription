{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch  # install steps: pytorch.org\n",
    "\n",
    "from tqdm.auto import tqdm  # !pip install tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "import urllib.parse as p\n",
    "import pandas as pd\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import json\n",
    "import subprocess\n",
    "import requests\n",
    "import re\n",
    "import youtube_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube  # !pip install pytube\n",
    "from pytube.exceptions import RegexMatchError\n",
    "from tqdm.auto import tqdm  # !pip install tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Set-up APIs & Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load youtube api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable OAuthlib's HTTPS verification when running locally.\n",
    "# *DO NOT* leave this option enabled in production.\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = os.getenv('DEVELOPER_KEY')\n",
    "\n",
    "os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey = DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load firebase credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "\n",
    "# Use a service account.\n",
    "cred = credentials.Certificate('../analytics-c4f16-firebase-adminsdk-plgdk-2a3d65b3a8.json')\n",
    "\n",
    "app = firebase_admin.initialize_app(cred)\n",
    "\n",
    "db = firestore.client(app)\n",
    "batch = db.batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load enviroment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = whisper.load_model(\"base\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define functons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I use this for savng to JSON files (i save to JSON in case I don't sent to my database properly, so I don't have to wait for a new load)\n",
    "def obj_dict(obj):\n",
    "    return obj.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nested_dict_keys(dic, keys):\n",
    "    d = dic\n",
    "    for key in keys:\n",
    "        if key in d.keys():\n",
    "            d = d[key]\n",
    "            continue\n",
    "        else:\n",
    "            return dic['snippet']['thumbnails']['default']['url'] \n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseHelper():\n",
    "    \n",
    "    def __init__(self, db):\n",
    "        self.db = db\n",
    "        self.CHANNELS = None\n",
    "        self.VIDEOS = {}\n",
    "    \n",
    "    def get_channel_db(self,db):\n",
    "        docs = self.db.collection(u'channels').stream()\n",
    "        channelsCollection = {}\n",
    "        for doc in docs:\n",
    "            channelsCollection[doc.id]=doc.to_dict()\n",
    "        return channelsCollection\n",
    "    \n",
    "    def set_channels(self):\n",
    "        self.CHANNELS = self.get_channel_db(db)\n",
    "        \n",
    "    def set_videos(self,_id,videos):\n",
    "        self.VIDEOS[_id] = videos\n",
    "\n",
    "    def get_channel_videos(self,channelId):\n",
    "\n",
    "        docs = db.collection(u'Videos').order_by('publishedAt', direction=firestore.Query.DESCENDING).where(u\"channelID\",u\"==\",u\"{}\".format(channelId)).stream()\n",
    "\n",
    "        collection = []\n",
    "        for doc in docs:\n",
    "            vid =doc.to_dict()\n",
    "            if \"#shorts\" not in vid['title']:\n",
    "                collection.append(vid)\n",
    "        return collection\n",
    "    \n",
    "    def get_channel_doc(self, video_url, allChannels):\n",
    "        channel_id = video_url_to_channel_id(video_url)\n",
    "        print(channel_id)\n",
    "\n",
    "        for record in allChannels.values():\n",
    "            if record['channelId'] == channel_id:\n",
    "                print(\"Channel {} already exists.\".format(record['title']))\n",
    "                return record\n",
    "\n",
    "        channelToAdd = generate_channel_json(video_url)    \n",
    "\n",
    "\n",
    "        add = input(\"Create channel for url ? (0 for no, any other key for yes)\".format(channelToAdd['title']))\n",
    "\n",
    "        if add == 0:\n",
    "            print(\"No channel for url\")\n",
    "            return\n",
    "\n",
    "\n",
    "        docRef = db.collection(u\"channels\").document(u\"{}\".format(channelToAdd['channelId'])).set(channelToAdd)\n",
    "        print(\"Channel {} is created.\".format(channelToAdd['title']))\n",
    "        return channelToAd\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_channel_json(sourceValue, source=\"url\"):\n",
    "    \n",
    "    if source==\"id\":\n",
    "        channel_id = sourceValue\n",
    "    elif source==\"url\":\n",
    "        video_url = sourceValue\n",
    "        channel_id = video_url_to_channel_id(video_url)\n",
    "    else:\n",
    "        raise Exception(\"{} not a valid source.\".format(source))\n",
    "            \n",
    "    request = youtube.channels().list(\n",
    "            part=\"snippet,contentDetails\",\n",
    "            id=channel_id\n",
    "        )\n",
    "    response = request.execute()\n",
    "    \n",
    "    channel_df = {}#pd.DataFrame(columns=['channelId', 'title', 'uploads','description','thumbnail'])\n",
    "    channel_df['channelId'] = channel_id\n",
    "    channel_df['title'] = response['items'][0]['snippet']['title']\n",
    "    channel_df['uploads'] = response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    channel_df['description'] = response['items'][0]['snippet']['description']\n",
    "    channel_df['thumbnail'] = check_nested_dict_keys(response['items'][0],[\"snippet\",\"thumbnails\",\"high\",\"url\"])\n",
    "    channel_df['videos'] = []\n",
    "    \n",
    "    #full_channel_df = pd.concat([full_channel_df, pd.DataFrame.from_dict(channel_df)])\n",
    "\n",
    "    return channel_df\n",
    "\n",
    "def video_url_to_channel_id(video_url):\n",
    "    video_param_from_url = video_url.split(\"/watch?v=\")[1].split(\"&\")[0]\n",
    "    request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails\",\n",
    "            id=video_param_from_url\n",
    "        )\n",
    "    response = request.execute()\n",
    "    channel_id = response['items'][0]['snippet']['channelId']\n",
    "    return channel_id\n",
    "\n",
    "\n",
    "def get_channel_doc(video_url, allChannels):\n",
    "    channel_id = video_url_to_channel_id(video_url)\n",
    "    print(channel_id)\n",
    "    \n",
    "    for record in allChannels.values():\n",
    "        if record['channelId'] == channel_id:\n",
    "            print(\"Channel {} already exists.\".format(record['title']))\n",
    "            return record\n",
    "\n",
    "    channelToAdd = generate_channel_json(video_url)    \n",
    "\n",
    "\n",
    "    add = input(\"Create channel for url ? (0 for no, any other key for yes)\".format(channelToAdd['title']))\n",
    "\n",
    "    if add == 0:\n",
    "        print(\"No channel for url\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    docRef = db.collection(u\"channels\").document(u\"{}\".format(channelToAdd['channelId'])).set(channelToAdd)\n",
    "    print(\"Channel {} is created.\".format(channelToAdd['title']))\n",
    "    return channelToAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uploads_id(youtube, channel_id):\n",
    "    request = youtube.channels().list(\n",
    "            part=\"snippet,contentDetails\",\n",
    "            id=channel_id\n",
    "        )\n",
    "    response = request.execute()\n",
    "    return response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "\n",
    "def get_uploaded_videos_response(youtube, channel_id,**args):\n",
    "    \n",
    "    playlist_id = get_uploads_id(youtube, channel_id)\n",
    "    max_total=args.get(\"max_total\") if args.get(\"max_total\") else 1000\n",
    "    oldest_date=args.get(\"oldest_date\") if args.get(\"oldest_date\") else False\n",
    "\n",
    "    \n",
    "    params = {\n",
    "        \"part\":\"snippet,contentDetails\",\n",
    "        \"playlistId\": playlist_id,\n",
    "        \"maxResults\": 50\n",
    "    }\n",
    "    \n",
    "    uploaded_videos_content_details_list = []\n",
    "\n",
    "    while True:\n",
    "        response = youtube.playlistItems().list(**params).execute()\n",
    "        \n",
    "        \n",
    "        if oldest_date:\n",
    "                \n",
    "            oldestResponseDate = response.get('items')[-1]['snippet']['publishedAt'].split(\"T\")[0]\n",
    "            if oldestResponseDate <= oldest_date:\n",
    "                add_response = []\n",
    "                for r in response.get('items'):\n",
    "                    date = r['snippet']['publishedAt'].split(\"T\")[0]\n",
    "                    if date <= oldest_date:\n",
    "                        break\n",
    "                    else:\n",
    "                        add_response.append(r)\n",
    "                videos_list = uploaded_videos_content_details_list + add_response\n",
    "                print(\"New videos found: {}\".format(len(videos_list)))\n",
    "                print(\"Newest video: {}, {}\".format(videos_list[0]['snippet']['title'],videos_list[0]['snippet']['publishedAt']))\n",
    "                print(\"Oldest video: {}, {}\".format(videos_list[-1]['snippet']['title'],videos_list[-1]['snippet']['publishedAt']))\n",
    "\n",
    "\n",
    "                return videos_list\n",
    "\n",
    "      \n",
    "        uploaded_videos_content_details_list = uploaded_videos_content_details_list + response.get('items')\n",
    "        \n",
    "        if (max_total and len(uploaded_videos_content_details_list) >= max_total):\n",
    "            return uploaded_videos_content_details_list[:max_total]\n",
    "     \n",
    "        elif 'nextPageToken' in response.keys():\n",
    "            params['pageToken'] = response['nextPageToken']\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(\"Videos retreived.\")\n",
    "    return uploaded_videos_content_details_list\n",
    "\n",
    "def getYoutubeDuration(videoId):\n",
    "    responseVideoDetails = youtube.videos().list( part=\"contentDetails\",id=videoId).execute()\n",
    "    durationResponse=responseVideoDetails['items'][0]['contentDetails']['duration']\n",
    "\n",
    "    duration_string = durationResponse.replace('PT',\"\")\n",
    "    number_values = re.findall('\\d+',duration_string)\n",
    "    symbols_available= ''.join([i for i in duration_string if not i.isdigit()])\n",
    "    symbol_map = {}\n",
    "    for symbol in 'HMS':\n",
    "        index = symbols_available.find(symbol)\n",
    "        if index > -1:\n",
    "            symbol_map[symbol] = number_values[index]\n",
    "\n",
    "    duration = 0\n",
    "\n",
    "\n",
    "    for idx in symbol_map:\n",
    "        if idx == \"H\":\n",
    "            duration = int(symbol_map[idx])*60*60 + duration\n",
    "        if idx == \"M\":\n",
    "            duration = int(symbol_map[idx])*60 + duration\n",
    "        if idx == \"S\":\n",
    "            duration = int(symbol_map[idx]) + duration\n",
    "            \n",
    "    return duration\n",
    "\n",
    "def video_response_list_to_list_of_dicts(response_list):\n",
    "    \n",
    "    list_of_dicts = []\n",
    "    for x in response_list:\n",
    "        if \"#shorts\" in x['snippet']['title'].lower():\n",
    "            continue\n",
    "            \n",
    "        video_df = {}\n",
    "        video_df['id'] = x['id']\n",
    "        video_df['videoId'] = x['contentDetails']['videoId'] \n",
    "        video_df['title'] = x['snippet']['title'] \n",
    "        video_df['description'] = x['snippet']['description'] \n",
    "        video_df['thumbnail'] = check_nested_dict_keys(x,[\"snippet\",\"thumbnails\",\"maxres\",\"url\"])      \n",
    "        video_df['channelID'] = x['snippet']['channelId'] \n",
    "        video_df['videoId'] = x['contentDetails']['videoId'] \n",
    "        video_df['publishedAt'] =x['snippet']['publishedAt'].split(\"T\")[0] \n",
    "        video_df['videoUrl'] = \"https://www.youtube.com/watch?v={}\".format(x['contentDetails']['videoId'] )\n",
    "        video_df['duration'] = getYoutubeDuration(x['contentDetails']['videoId']  )\n",
    "        \n",
    "        video_df['text'] = ''\n",
    "        list_of_dicts.append(video_df)\n",
    "\n",
    "\n",
    "    return list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullChannel(url, DB, oldest_date=None):\n",
    "\n",
    "    #Create channel\n",
    "    channel = get_channel_doc(url,DB.CHANNELS)\n",
    "    \n",
    "    videos_collections= DB.get_channel_videos(channel['channelId'])#load all videos\n",
    "    \n",
    "    DB.set_videos(channel['channelId'],videos_collections)\n",
    "    #check for videos not yet uploaded\n",
    "\n",
    "    if videos_collections and oldest_date:\n",
    "        oldest_date = videos_collections[0]['publishedAt']\n",
    "    else:\n",
    "        oldest_date = None\n",
    "    new_video_responses = get_uploaded_videos_response(youtube, channel['channelId'],\n",
    "                                               oldest_date=oldest_date)\n",
    "\n",
    "    list_of_new_video_responses_as_dicts = video_response_list_to_list_of_dicts(new_video_responses)\n",
    "\n",
    "\n",
    "    return channel, list_of_new_video_responses_as_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCost(videoList):\n",
    "    cost = 0\n",
    "    for vid in videoList:\n",
    "        cost = vid['duration']*0.00025 + cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcript_object(video, text, sentences, result,source):\n",
    "    item = {}\n",
    "    item['videoId'] = video['videoId']\n",
    "    item['transcriptId'] = 'TRANSCRIPT_'+video['videoId']\n",
    "    \n",
    "    keys = ['start','text','id']\n",
    "    segments = sentences\n",
    "    for i,s in zip(range(len(segments)),segments):\n",
    "        s['id'] = i\n",
    "    item['transcript'] = [{ keep: item[keep] for keep in keys } for item,i in zip(segments,range(len(segments)) )]\n",
    "    item['fullTranscript'] = result\n",
    "    item['text'] = text\n",
    "    item['source'] = source\n",
    "\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(url,_id):\n",
    "    yt = YouTube(url)\n",
    "    video = yt.streams.filter(only_audio=True).first()\n",
    "    out_file=video.download(output_path=\"audio_files\")\n",
    "    base, ext = os.path.splitext(out_file)\n",
    "    new_file = 'audio_files/'+_id+'.mp3'\n",
    "    os.rename(out_file, new_file)\n",
    "    a = new_file\n",
    "    return a\n",
    "\n",
    "def transcribe(video):\n",
    "    \n",
    "    metric = {}\n",
    "    result = {}\n",
    "    _id = video['videoId']\n",
    "\n",
    "    \n",
    "    source = \"\"\n",
    "    try:\n",
    "        start = time.time()\n",
    "\n",
    "        get_audio(video['videoUrl'],_id)\n",
    "        print('Downloaded {}'.format(video['videoId']))\n",
    "\n",
    "\n",
    "        # transcribe to get speech-to-text data\n",
    "        result = model.transcribe('audio_files/{}.mp3'.format(_id))\n",
    "\n",
    "        # add results to data list\n",
    "        with open('transcript_files/{}.json'.format(_id), 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "        os.remove(\"audio_files/{}.mp3\".format(_id))\n",
    "        source = \"whisper\"\n",
    "        print('Transcribed {}'.format(video['videoId']))\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e: # work on python 3.x\n",
    "\n",
    "        print(\"video error: {}\".format(_id))\n",
    "\n",
    "     \n",
    "        try:\n",
    "            print('Could not get streaming data. Attempting assembly...')\n",
    "            ydl_opts = {'format': 'bestaudio'}\n",
    "\n",
    "\n",
    "            ydl =youtube_dl.YoutubeDL(ydl_opts)\n",
    "\n",
    "            videoUrl = video['videoUrl']\n",
    "            info = ydl.extract_info(videoUrl, download=False)\n",
    "            audioUrl = info['formats'][0]['url']\n",
    "            requestForTrancript = postTranscript(audioUrl)\n",
    "            source = \"pendng\"\n",
    "        except Exception as e: # work on python 3.x\n",
    "            print('Could not get assembly data.')\n",
    "\n",
    "            source = \"fail\"\n",
    "\n",
    "\n",
    "    \n",
    "    metric['id'] = _id\n",
    "    metric['time'] = time.time() - start\n",
    "    \n",
    "    return result, metric,source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = DatabaseHelper(db)\n",
    "DB.set_channels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCamLstJyCa-t5gfZegxsFMw\n",
      "Channel Colin and Samir already exists.\n",
      "Videos retreived.\n"
     ]
    }
   ],
   "source": [
    "url= \"https://www.youtube.com/watch?v=lBCOOTyU46M&t=512s\"\n",
    "channel, new_videos = pullChannel(url,DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = DB.db.collection(u'Transcripts').stream()\n",
    "transcripts_list = []\n",
    "for doc in docs:\n",
    "    try:\n",
    "        transcripts_list.append(doc.to_dict()['videoId'])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = done_1+done_2+done_3+error+transcripts_list\n",
    "l = [v for v in new_videos if ((v['videoId']not in done )and not (v['text'] == 'error'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TranscribeList(l):\n",
    "    data = []\n",
    "    metrics= []\n",
    "    done = []\n",
    "    i=0\n",
    "    for doc in l:\n",
    "\n",
    "        if doc['videoId'] not in done:\n",
    "            print(i)\n",
    "        else:\n",
    "            print(\"already transcribed.\")\n",
    "            continue\n",
    "        i = i+1\n",
    "\n",
    "        result, metric, source = transcribe(doc)\n",
    "        \n",
    "        if source == \"yta\":\n",
    "\n",
    "            doc['text'] = \". \".join([r['text'] for r in result])\n",
    "            transcript = transcript_object(doc, doc['text'], result,result, source)\n",
    "\n",
    "            docRef = DB.db.collection(u\"Videos\").document(u\"{}\".format(doc['videoId'])).set(doc)\n",
    "            docRef = DB.db.collection(u\"Transcripts\").document(u\"{}\".format(doc['videoId'])).set(transcript)\n",
    "            print(metric['time'],\" -- \",doc[\"duration\"])\n",
    "\n",
    "            \n",
    "        if source == \"whisper\":\n",
    "            data.append(result)\n",
    "            metrics.append(metric)\n",
    "\n",
    "\n",
    "      \n",
    "            doc['text'] = result['text']\n",
    "            \n",
    "            transcript = transcript_object(doc, result['text'], result['sentences'],result, source)\n",
    "\n",
    "            docRef = DB.db.collection(u\"Videos\").document(u\"{}\".format(doc['videoId'])).set(doc)\n",
    "            docRef = DB.db.collection(u\"Transcripts\").document(u\"{}\".format(doc['videoId'])).set(transcript)\n",
    "\n",
    "            done.append(doc)\n",
    "            print(metric['time'],\" -- \",doc[\"duration\"])\n",
    "\n",
    "\n",
    "        else:\n",
    "            doc['text'] = 'error'\n",
    "            #transcript = transcript_object(doc, result['segments'], source)\n",
    "\n",
    "            docRef = DB.db.collection(u\"Videos\").document(u\"{}\".format(doc['videoId'])).set(doc)\n",
    "    #         docRef = DB.db.collection(u\"Transcripts\").document(u\"{}\".format(doc['videoId'])).set(transcript)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TranscribeList(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
